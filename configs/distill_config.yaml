# Configuration for BLIP distillation training

# Model configuration
model:
  vision_model_name: "google/vit-base-patch16-224-in21k"
  text_model_name: "bert-base-uncased"
  vision_hidden_size: 768
  text_hidden_size: 768
  cross_attention_dim: 768
  num_visual_encoder_layers: 6 # Reduced from 12 in original ViT
  num_text_encoder_layers: 6 # Reduced from 12 in original text model
  num_text_decoder_layers: 6 # Reduced from 12 in original text model
  num_attention_heads: 8 # Reduced from 12 in original
  intermediate_size: 2048 # Reduced from 3072 in original
  vocab_size: 30524 # Match teacher model's vocabulary size

# Distillation configuration
distillation:
  temperature: 2.0
  alpha: 0.5 # Weight balance between soft and hard targets
  lambda_logits: 1.0 # Weight for logits distillation
  lambda_feature: 0.5 # Weight for feature distillation
  lambda_attn: 0.5 # Weight for attention distillation
  use_feature_distillation: true # Enable for better knowledge transfer
  use_attn_distillation: true # Enable for better knowledge transfer

# Data configuration
data:
  dataset: "coco"
  batch_size: 16 # Increased for faster training
  max_length: 30
  num_workers: 8 # Increased to speed up data loading
  image_size: 384
  subset_size: null # Set to a number like 10000 for faster debugging

# Training configuration
training:
  num_epochs: 10
  learning_rate: 0.0001
  weight_decay: 0.01
  warmup_steps: 1000
  mixed_precision: true  # Enable mixed precision for faster training with CUDA
  device: "cuda"  # Use CUDA for GPU acceleration
  save_steps: 1  # Save checkpoints every epoch
  eval_steps: 1  # Evaluate every epoch
  gradient_accumulation_steps: 4  # Accumulate gradients for effective batch size of 64
  max_grad_norm: 1.0
  fp16_opt_level: "O2"  # More aggressive mixed precision optimization
  use_amp: true  # Use automatic mixed precision
  use_gradient_checkpointing: true  # Save memory at the cost of some speed
  dataloader_pin_memory: true  # Speed up data transfer to GPU
  dataloader_prefetch_factor: 2  # Prefetch batches
  use_compile: true  # Use torch.compile() for faster training (PyTorch 2.0+)
